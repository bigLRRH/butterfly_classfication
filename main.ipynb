{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14639b20-4e9b-4389-99d1-3cebc1bd6b8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T08:20:37.205421Z",
     "iopub.status.busy": "2023-06-18T08:20:37.203881Z",
     "iopub.status.idle": "2023-06-18T08:20:37.210569Z",
     "shell.execute_reply": "2023-06-18T08:20:37.209406Z",
     "shell.execute_reply.started": "2023-06-18T08:20:37.205363Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "from paddle.io import Dataset\n",
    "import paddle\n",
    "from paddle.vision.transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88eb926d-714b-4536-9a92-f0d4116e8a98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T08:20:37.213303Z",
     "iopub.status.busy": "2023-06-18T08:20:37.212584Z",
     "iopub.status.idle": "2023-06-18T08:20:37.218700Z",
     "shell.execute_reply": "2023-06-18T08:20:37.217726Z",
     "shell.execute_reply.started": "2023-06-18T08:20:37.213273Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_path = 'work/datas/Butterfly20/Butterfly20'\n",
    "target_path = ''\n",
    "label_path = 'work/datas/Butterfly20/data_list.txt'\n",
    "test_data_dir='work/datas/Butterfly20_test'\n",
    "test_path='work/datas/Butterfly20_test/testpath.txt'\n",
    "spicies_path = 'work/datas/Butterfly20/species.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34f8a09e-7346-4964-8272-e0fa6c513f1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T08:20:37.221012Z",
     "iopub.status.busy": "2023-06-18T08:20:37.220194Z",
     "iopub.status.idle": "2023-06-18T08:20:37.248470Z",
     "shell.execute_reply": "2023-06-18T08:20:37.247424Z",
     "shell.execute_reply.started": "2023-06-18T08:20:37.220973Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 224, 224)\r\n",
      "1866\r\n"
     ]
    }
   ],
   "source": [
    "#数据预处理\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    步骤一：继承 paddle.io.Dataset 类\n",
    "    \"\"\"\n",
    "    def __init__(self, label_path, transform=None):\n",
    "        \"\"\"\n",
    "        步骤二：实现 __init__ 函数，初始化数据集，将样本和标签映射到列表中\n",
    "        \"\"\"\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.data_list = []\n",
    "        with open(label_path,encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                image_path,genus_label,species_label = line.strip('\\n').split(' ')\n",
    "                self.data_list.append([image_path,species_label])\n",
    "        # 2. 传入定义好的数据处理方法，作为自定义数据集类的一个属性\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        步骤三：实现 __getitem__ 函数，定义指定 index 时如何获取数据，并返回单条数据（样本数据、对应的标签）\n",
    "        \"\"\"\n",
    "        image_path, label = self.data_list[index]\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        image = image.astype('float32')\n",
    "        # 3. 应用数据处理方法到图像上\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        label = int(label)-1\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        步骤四：实现 __len__ 函数，返回数据集的样本总数\n",
    "        \"\"\"\n",
    "        return len(self.data_list)\n",
    "\n",
    "# 数据预处理和数据增强\n",
    "transform_train = Compose([\n",
    "    RandomRotation(40),\n",
    "    RandomHorizontalFlip(0.4),\n",
    "    RandomVerticalFlip(0.1),\n",
    "    Resize(size=(224, 224)),\n",
    "    Normalize(mean=[127.5, 127.5, 127.5], std=[127.5, 127.5, 127.5],data_format='HWC'),\n",
    "    Transpose()])\n",
    "\n",
    "#数据加载\n",
    "train_dataset = MyDataset(label_path, transform_train)\n",
    "print(train_dataset[0].__getitem__(0).shape)\n",
    "print(train_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c073cad7-c6ad-45ee-9b35-1aa50640c65e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T08:20:37.250663Z",
     "iopub.status.busy": "2023-06-18T08:20:37.250134Z",
     "iopub.status.idle": "2023-06-18T08:20:37.375726Z",
     "shell.execute_reply": "2023-06-18T08:20:37.374312Z",
     "shell.execute_reply.started": "2023-06-18T08:20:37.250620Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\r\n",
      "   Layer (type)         Input Shape          Output Shape         Param #    \r\n",
      "===============================================================================\r\n",
      "    Conv2D-213       [[1, 3, 224, 224]]   [1, 64, 112, 112]        9,408     \r\n",
      "  BatchNorm2D-213   [[1, 64, 112, 112]]   [1, 64, 112, 112]         256      \r\n",
      "      ReLU-69       [[1, 64, 112, 112]]   [1, 64, 112, 112]          0       \r\n",
      "    MaxPool2D-5     [[1, 64, 112, 112]]    [1, 64, 56, 56]           0       \r\n",
      "    Conv2D-215       [[1, 64, 56, 56]]     [1, 64, 56, 56]         4,096     \r\n",
      "  BatchNorm2D-215    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \r\n",
      "      ReLU-70        [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \r\n",
      "    Conv2D-216       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864     \r\n",
      "  BatchNorm2D-216    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \r\n",
      "    Conv2D-217       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384     \r\n",
      "  BatchNorm2D-217    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \r\n",
      "    Conv2D-214       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384     \r\n",
      "  BatchNorm2D-214    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \r\n",
      "BottleneckBlock-65   [[1, 64, 56, 56]]     [1, 256, 56, 56]          0       \r\n",
      "    Conv2D-218       [[1, 256, 56, 56]]    [1, 64, 56, 56]        16,384     \r\n",
      "  BatchNorm2D-218    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \r\n",
      "      ReLU-71        [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \r\n",
      "    Conv2D-219       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864     \r\n",
      "  BatchNorm2D-219    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \r\n",
      "    Conv2D-220       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384     \r\n",
      "  BatchNorm2D-220    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \r\n",
      "BottleneckBlock-66   [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \r\n",
      "    Conv2D-221       [[1, 256, 56, 56]]    [1, 64, 56, 56]        16,384     \r\n",
      "  BatchNorm2D-221    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \r\n",
      "      ReLU-72        [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \r\n",
      "    Conv2D-222       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864     \r\n",
      "  BatchNorm2D-222    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \r\n",
      "    Conv2D-223       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384     \r\n",
      "  BatchNorm2D-223    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \r\n",
      "BottleneckBlock-67   [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \r\n",
      "    Conv2D-225       [[1, 256, 56, 56]]    [1, 128, 56, 56]       32,768     \r\n",
      "  BatchNorm2D-225    [[1, 128, 56, 56]]    [1, 128, 56, 56]         512      \r\n",
      "      ReLU-73        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \r\n",
      "    Conv2D-226       [[1, 128, 56, 56]]    [1, 128, 28, 28]       147,456    \r\n",
      "  BatchNorm2D-226    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \r\n",
      "    Conv2D-227       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536     \r\n",
      "  BatchNorm2D-227    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \r\n",
      "    Conv2D-224       [[1, 256, 56, 56]]    [1, 512, 28, 28]       131,072    \r\n",
      "  BatchNorm2D-224    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \r\n",
      "BottleneckBlock-68   [[1, 256, 56, 56]]    [1, 512, 28, 28]          0       \r\n",
      "    Conv2D-228       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536     \r\n",
      "  BatchNorm2D-228    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \r\n",
      "      ReLU-74        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \r\n",
      "    Conv2D-229       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456    \r\n",
      "  BatchNorm2D-229    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \r\n",
      "    Conv2D-230       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536     \r\n",
      "  BatchNorm2D-230    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \r\n",
      "BottleneckBlock-69   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \r\n",
      "    Conv2D-231       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536     \r\n",
      "  BatchNorm2D-231    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \r\n",
      "      ReLU-75        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \r\n",
      "    Conv2D-232       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456    \r\n",
      "  BatchNorm2D-232    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \r\n",
      "    Conv2D-233       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536     \r\n",
      "  BatchNorm2D-233    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \r\n",
      "BottleneckBlock-70   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \r\n",
      "    Conv2D-234       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536     \r\n",
      "  BatchNorm2D-234    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \r\n",
      "      ReLU-76        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \r\n",
      "    Conv2D-235       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456    \r\n",
      "  BatchNorm2D-235    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \r\n",
      "    Conv2D-236       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536     \r\n",
      "  BatchNorm2D-236    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \r\n",
      "BottleneckBlock-71   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \r\n",
      "    Conv2D-238       [[1, 512, 28, 28]]    [1, 256, 28, 28]       131,072    \r\n",
      "  BatchNorm2D-238    [[1, 256, 28, 28]]    [1, 256, 28, 28]        1,024     \r\n",
      "      ReLU-77       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \r\n",
      "    Conv2D-239       [[1, 256, 28, 28]]    [1, 256, 14, 14]       589,824    \r\n",
      "  BatchNorm2D-239    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \r\n",
      "    Conv2D-240       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \r\n",
      "  BatchNorm2D-240   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \r\n",
      "    Conv2D-237       [[1, 512, 28, 28]]   [1, 1024, 14, 14]       524,288    \r\n",
      "  BatchNorm2D-237   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \r\n",
      "BottleneckBlock-72   [[1, 512, 28, 28]]   [1, 1024, 14, 14]          0       \r\n",
      "    Conv2D-241      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \r\n",
      "  BatchNorm2D-241    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \r\n",
      "      ReLU-78       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \r\n",
      "    Conv2D-242       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \r\n",
      "  BatchNorm2D-242    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \r\n",
      "    Conv2D-243       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \r\n",
      "  BatchNorm2D-243   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \r\n",
      "BottleneckBlock-73  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \r\n",
      "    Conv2D-244      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \r\n",
      "  BatchNorm2D-244    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \r\n",
      "      ReLU-79       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \r\n",
      "    Conv2D-245       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \r\n",
      "  BatchNorm2D-245    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \r\n",
      "    Conv2D-246       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \r\n",
      "  BatchNorm2D-246   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \r\n",
      "BottleneckBlock-74  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \r\n",
      "    Conv2D-247      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \r\n",
      "  BatchNorm2D-247    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \r\n",
      "      ReLU-80       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \r\n",
      "    Conv2D-248       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \r\n",
      "  BatchNorm2D-248    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \r\n",
      "    Conv2D-249       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \r\n",
      "  BatchNorm2D-249   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \r\n",
      "BottleneckBlock-75  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \r\n",
      "    Conv2D-250      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \r\n",
      "  BatchNorm2D-250    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \r\n",
      "      ReLU-81       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \r\n",
      "    Conv2D-251       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \r\n",
      "  BatchNorm2D-251    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \r\n",
      "    Conv2D-252       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \r\n",
      "  BatchNorm2D-252   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \r\n",
      "BottleneckBlock-76  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \r\n",
      "    Conv2D-253      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \r\n",
      "  BatchNorm2D-253    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \r\n",
      "      ReLU-82       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \r\n",
      "    Conv2D-254       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \r\n",
      "  BatchNorm2D-254    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \r\n",
      "    Conv2D-255       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \r\n",
      "  BatchNorm2D-255   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \r\n",
      "BottleneckBlock-77  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \r\n",
      "    Conv2D-257      [[1, 1024, 14, 14]]    [1, 512, 14, 14]       524,288    \r\n",
      "  BatchNorm2D-257    [[1, 512, 14, 14]]    [1, 512, 14, 14]        2,048     \r\n",
      "      ReLU-83        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \r\n",
      "    Conv2D-258       [[1, 512, 14, 14]]     [1, 512, 7, 7]       2,359,296   \r\n",
      "  BatchNorm2D-258     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \r\n",
      "    Conv2D-259        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576   \r\n",
      "  BatchNorm2D-259    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \r\n",
      "    Conv2D-256      [[1, 1024, 14, 14]]    [1, 2048, 7, 7]       2,097,152   \r\n",
      "  BatchNorm2D-256    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \r\n",
      "BottleneckBlock-78  [[1, 1024, 14, 14]]    [1, 2048, 7, 7]           0       \r\n",
      "    Conv2D-260       [[1, 2048, 7, 7]]      [1, 512, 7, 7]       1,048,576   \r\n",
      "  BatchNorm2D-260     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \r\n",
      "      ReLU-84        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \r\n",
      "    Conv2D-261        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296   \r\n",
      "  BatchNorm2D-261     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \r\n",
      "    Conv2D-262        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576   \r\n",
      "  BatchNorm2D-262    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \r\n",
      "BottleneckBlock-79   [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \r\n",
      "    Conv2D-263       [[1, 2048, 7, 7]]      [1, 512, 7, 7]       1,048,576   \r\n",
      "  BatchNorm2D-263     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \r\n",
      "      ReLU-85        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \r\n",
      "    Conv2D-264        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296   \r\n",
      "  BatchNorm2D-264     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \r\n",
      "    Conv2D-265        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576   \r\n",
      "  BatchNorm2D-265    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \r\n",
      "BottleneckBlock-80   [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \r\n",
      "AdaptiveAvgPool2D-5  [[1, 2048, 7, 7]]     [1, 2048, 1, 1]           0       \r\n",
      "     Linear-5           [[1, 2048]]            [1, 20]            40,980     \r\n",
      "===============================================================================\r\n",
      "Total params: 23,602,132\r\n",
      "Trainable params: 23,495,892\r\n",
      "Non-trainable params: 106,240\r\n",
      "-------------------------------------------------------------------------------\r\n",
      "Input size (MB): 0.57\r\n",
      "Forward/backward pass size (MB): 261.48\r\n",
      "Params size (MB): 90.03\r\n",
      "Estimated Total Size (MB): 352.09\r\n",
      "-------------------------------------------------------------------------------\r\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 23602132, 'trainable_params': 23495892}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型组网\n",
    "res50 = paddle.vision.models.resnet50(num_classes=20)\n",
    "paddle.summary(res50,(1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a5b424b-8aa5-491a-86ce-7254cb803f3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T08:20:37.379775Z",
     "iopub.status.busy": "2023-06-18T08:20:37.379084Z",
     "iopub.status.idle": "2023-06-18T08:20:37.385742Z",
     "shell.execute_reply": "2023-06-18T08:20:37.384641Z",
     "shell.execute_reply.started": "2023-06-18T08:20:37.379745Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#封装模型\n",
    "model = paddle.Model(res50)\n",
    "#调参\n",
    "#paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters(), weight_decay=0.01)\n",
    "#paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())\n",
    "model.prepare(optimizer=paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters()), \n",
    "              loss=paddle.nn.CrossEntropyLoss(), \n",
    "              metrics=paddle.metric.Accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58066f14-bbe1-4982-ab84-3d4e0b783cdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T08:20:37.387243Z",
     "iopub.status.busy": "2023-06-18T08:20:37.386738Z",
     "iopub.status.idle": "2023-06-18T09:39:09.696463Z",
     "shell.execute_reply": "2023-06-18T09:39:09.695418Z",
     "shell.execute_reply.started": "2023-06-18T08:20:37.387217Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\r\n",
      "Epoch 1/256\r\n",
      "step 59/59 [==============================] - loss: 2.8281 - acc: 0.1329 - 312ms/step          \r\n",
      "Epoch 2/256\r\n",
      "step 59/59 [==============================] - loss: 2.4405 - acc: 0.2315 - 307ms/step          \r\n",
      "Epoch 3/256\r\n",
      "step 59/59 [==============================] - loss: 2.1708 - acc: 0.2755 - 307ms/step          \r\n",
      "Epoch 4/256\r\n",
      "step 59/59 [==============================] - loss: 1.8596 - acc: 0.3124 - 307ms/step          \r\n",
      "Epoch 5/256\r\n",
      "step 59/59 [==============================] - loss: 2.5606 - acc: 0.3666 - 309ms/step          \r\n",
      "Epoch 6/256\r\n",
      "step 59/59 [==============================] - loss: 1.6269 - acc: 0.3864 - 312ms/step          \r\n",
      "Epoch 7/256\r\n",
      "step 59/59 [==============================] - loss: 1.7179 - acc: 0.4191 - 314ms/step          \r\n",
      "Epoch 8/256\r\n",
      "step 59/59 [==============================] - loss: 1.8876 - acc: 0.4502 - 314ms/step          \r\n",
      "Epoch 9/256\r\n",
      "step 59/59 [==============================] - loss: 2.3718 - acc: 0.4802 - 313ms/step          \r\n",
      "Epoch 10/256\r\n",
      "step 59/59 [==============================] - loss: 2.1002 - acc: 0.5150 - 311ms/step          \r\n",
      "Epoch 11/256\r\n",
      "step 59/59 [==============================] - loss: 1.2554 - acc: 0.5032 - 313ms/step          \r\n",
      "Epoch 12/256\r\n",
      "step 59/59 [==============================] - loss: 1.5362 - acc: 0.5466 - 307ms/step          \r\n",
      "Epoch 13/256\r\n",
      "step 59/59 [==============================] - loss: 2.2008 - acc: 0.5718 - 313ms/step          \r\n",
      "Epoch 14/256\r\n",
      "step 59/59 [==============================] - loss: 0.8975 - acc: 0.5820 - 319ms/step          \r\n",
      "Epoch 15/256\r\n",
      "step 59/59 [==============================] - loss: 1.1616 - acc: 0.5847 - 305ms/step          \r\n",
      "Epoch 16/256\r\n",
      "step 59/59 [==============================] - loss: 1.8142 - acc: 0.5868 - 305ms/step          \r\n",
      "Epoch 17/256\r\n",
      "step 59/59 [==============================] - loss: 1.8799 - acc: 0.6426 - 309ms/step          \r\n",
      "Epoch 18/256\r\n",
      "step 59/59 [==============================] - loss: 1.6355 - acc: 0.6377 - 308ms/step          \r\n",
      "Epoch 19/256\r\n",
      "step 59/59 [==============================] - loss: 1.1981 - acc: 0.6527 - 314ms/step          \r\n",
      "Epoch 20/256\r\n",
      "step 59/59 [==============================] - loss: 0.6673 - acc: 0.6618 - 312ms/step          \r\n",
      "Epoch 21/256\r\n",
      "step 59/59 [==============================] - loss: 0.8785 - acc: 0.7213 - 318ms/step          \r\n",
      "Epoch 22/256\r\n",
      "step 59/59 [==============================] - loss: 0.8217 - acc: 0.7229 - 312ms/step          \r\n",
      "Epoch 23/256\r\n",
      "step 59/59 [==============================] - loss: 1.1215 - acc: 0.7385 - 304ms/step          \r\n",
      "Epoch 24/256\r\n",
      "step 59/59 [==============================] - loss: 0.6682 - acc: 0.7262 - 316ms/step          \r\n",
      "Epoch 25/256\r\n",
      "step 59/59 [==============================] - loss: 0.5589 - acc: 0.7213 - 309ms/step          \r\n",
      "Epoch 26/256\r\n",
      "step 59/59 [==============================] - loss: 1.2344 - acc: 0.7487 - 314ms/step          \r\n",
      "Epoch 27/256\r\n",
      "step 59/59 [==============================] - loss: 0.3525 - acc: 0.7535 - 307ms/step          \r\n",
      "Epoch 28/256\r\n",
      "step 59/59 [==============================] - loss: 0.3632 - acc: 0.7830 - 314ms/step          \r\n",
      "Epoch 29/256\r\n",
      "step 59/59 [==============================] - loss: 1.0539 - acc: 0.7572 - 303ms/step          \r\n",
      "Epoch 30/256\r\n",
      "step 59/59 [==============================] - loss: 0.3928 - acc: 0.8044 - 315ms/step          \r\n",
      "Epoch 31/256\r\n",
      "step 59/59 [==============================] - loss: 0.3922 - acc: 0.8269 - 315ms/step          \r\n",
      "Epoch 32/256\r\n",
      "step 59/59 [==============================] - loss: 0.7729 - acc: 0.8339 - 307ms/step          \r\n",
      "Epoch 33/256\r\n",
      "step 59/59 [==============================] - loss: 0.2302 - acc: 0.8044 - 299ms/step          \r\n",
      "Epoch 34/256\r\n",
      "step 59/59 [==============================] - loss: 0.6535 - acc: 0.8012 - 314ms/step          \r\n",
      "Epoch 35/256\r\n",
      "step 59/59 [==============================] - loss: 1.3709 - acc: 0.8232 - 318ms/step          \r\n",
      "Epoch 36/256\r\n",
      "step 59/59 [==============================] - loss: 0.6775 - acc: 0.8489 - 307ms/step          \r\n",
      "Epoch 37/256\r\n",
      "step 59/59 [==============================] - loss: 0.3114 - acc: 0.8424 - 306ms/step          \r\n",
      "Epoch 38/256\r\n",
      "step 59/59 [==============================] - loss: 1.2088 - acc: 0.8344 - 312ms/step          \r\n",
      "Epoch 39/256\r\n",
      "step 59/59 [==============================] - loss: 0.3196 - acc: 0.7990 - 305ms/step          \r\n",
      "Epoch 40/256\r\n",
      "step 59/59 [==============================] - loss: 0.6212 - acc: 0.8376 - 302ms/step          \r\n",
      "Epoch 41/256\r\n",
      "step 59/59 [==============================] - loss: 0.2271 - acc: 0.8601 - 310ms/step          \r\n",
      "Epoch 42/256\r\n",
      "step 59/59 [==============================] - loss: 0.2578 - acc: 0.8703 - 322ms/step          \r\n",
      "Epoch 43/256\r\n",
      "step 59/59 [==============================] - loss: 0.6646 - acc: 0.8917 - 311ms/step          \r\n",
      "Epoch 44/256\r\n",
      "step 59/59 [==============================] - loss: 0.5411 - acc: 0.8676 - 310ms/step          \r\n",
      "Epoch 45/256\r\n",
      "step 59/59 [==============================] - loss: 0.2918 - acc: 0.8698 - 310ms/step          \r\n",
      "Epoch 46/256\r\n",
      "step 59/59 [==============================] - loss: 0.8398 - acc: 0.8601 - 307ms/step          \r\n",
      "Epoch 47/256\r\n",
      "step 59/59 [==============================] - loss: 0.3090 - acc: 0.8655 - 313ms/step          \r\n",
      "Epoch 48/256\r\n",
      "step 59/59 [==============================] - loss: 0.7581 - acc: 0.8730 - 315ms/step          \r\n",
      "Epoch 49/256\r\n",
      "step 59/59 [==============================] - loss: 0.9079 - acc: 0.8725 - 308ms/step          \r\n",
      "Epoch 50/256\r\n",
      "step 59/59 [==============================] - loss: 0.0802 - acc: 0.8810 - 309ms/step          \r\n",
      "Epoch 51/256\r\n",
      "step 59/59 [==============================] - loss: 0.0482 - acc: 0.9035 - 309ms/step          \r\n",
      "Epoch 52/256\r\n",
      "step 59/59 [==============================] - loss: 0.3279 - acc: 0.8971 - 311ms/step          \r\n",
      "Epoch 53/256\r\n",
      "step 59/59 [==============================] - loss: 0.0820 - acc: 0.8896 - 313ms/step          \r\n",
      "Epoch 54/256\r\n",
      "step 59/59 [==============================] - loss: 0.7830 - acc: 0.9084 - 315ms/step          \r\n",
      "Epoch 55/256\r\n",
      "step 59/59 [==============================] - loss: 0.2083 - acc: 0.8773 - 320ms/step          \r\n",
      "Epoch 56/256\r\n",
      "step 59/59 [==============================] - loss: 0.6324 - acc: 0.9041 - 312ms/step          \r\n",
      "Epoch 57/256\r\n",
      "step 59/59 [==============================] - loss: 0.7972 - acc: 0.8923 - 319ms/step          \r\n",
      "Epoch 58/256\r\n",
      "step 59/59 [==============================] - loss: 1.5346 - acc: 0.8730 - 308ms/step          \r\n",
      "Epoch 59/256\r\n",
      "step 59/59 [==============================] - loss: 0.0534 - acc: 0.9019 - 300ms/step          \r\n",
      "Epoch 60/256\r\n",
      "step 59/59 [==============================] - loss: 0.0383 - acc: 0.9319 - 305ms/step          \r\n",
      "Epoch 61/256\r\n",
      "step 59/59 [==============================] - loss: 0.3360 - acc: 0.9068 - 310ms/step          \r\n",
      "Epoch 62/256\r\n",
      "step 59/59 [==============================] - loss: 0.5888 - acc: 0.9212 - 320ms/step          \r\n",
      "Epoch 63/256\r\n",
      "step 59/59 [==============================] - loss: 0.5695 - acc: 0.9094 - 308ms/step          \r\n",
      "Epoch 64/256\r\n",
      "step 59/59 [==============================] - loss: 0.3024 - acc: 0.9185 - 299ms/step          \r\n",
      "Epoch 65/256\r\n",
      "step 59/59 [==============================] - loss: 0.1857 - acc: 0.9196 - 309ms/step          \r\n",
      "Epoch 66/256\r\n",
      "step 59/59 [==============================] - loss: 0.1030 - acc: 0.9244 - 305ms/step          \r\n",
      "Epoch 67/256\r\n",
      "step 59/59 [==============================] - loss: 0.2483 - acc: 0.9212 - 306ms/step          \r\n",
      "Epoch 68/256\r\n",
      "step 59/59 [==============================] - loss: 0.1955 - acc: 0.9325 - 311ms/step          \r\n",
      "Epoch 69/256\r\n",
      "step 59/59 [==============================] - loss: 0.7061 - acc: 0.9411 - 318ms/step          \r\n",
      "Epoch 70/256\r\n",
      "step 59/59 [==============================] - loss: 0.0987 - acc: 0.9341 - 307ms/step          \r\n",
      "Epoch 71/256\r\n",
      "step 59/59 [==============================] - loss: 0.4704 - acc: 0.9416 - 308ms/step          \r\n",
      "Epoch 72/256\r\n",
      "step 59/59 [==============================] - loss: 1.7051 - acc: 0.9228 - 306ms/step          \r\n",
      "Epoch 73/256\r\n",
      "step 59/59 [==============================] - loss: 0.6878 - acc: 0.8826 - 308ms/step          \r\n",
      "Epoch 74/256\r\n",
      "step 59/59 [==============================] - loss: 0.4211 - acc: 0.8891 - 309ms/step          \r\n",
      "Epoch 75/256\r\n",
      "step 59/59 [==============================] - loss: 0.5383 - acc: 0.9362 - 311ms/step          \r\n",
      "Epoch 76/256\r\n",
      "step 59/59 [==============================] - loss: 1.6648 - acc: 0.9250 - 326ms/step          \r\n",
      "Epoch 77/256\r\n",
      "step 59/59 [==============================] - loss: 0.1713 - acc: 0.9416 - 311ms/step          \r\n",
      "Epoch 78/256\r\n",
      "step 59/59 [==============================] - loss: 0.1100 - acc: 0.9491 - 322ms/step          \r\n",
      "Epoch 79/256\r\n",
      "step 59/59 [==============================] - loss: 0.1273 - acc: 0.9416 - 327ms/step          \r\n",
      "Epoch 80/256\r\n",
      "step 59/59 [==============================] - loss: 0.0529 - acc: 0.9389 - 316ms/step          \r\n",
      "Epoch 81/256\r\n",
      "step 59/59 [==============================] - loss: 0.3952 - acc: 0.9384 - 313ms/step          \r\n",
      "Epoch 82/256\r\n",
      "step 59/59 [==============================] - loss: 0.6576 - acc: 0.9330 - 317ms/step          \r\n",
      "Epoch 83/256\r\n",
      "step 59/59 [==============================] - loss: 0.0886 - acc: 0.9459 - 320ms/step          \r\n",
      "Epoch 84/256\r\n",
      "step 59/59 [==============================] - loss: 0.0907 - acc: 0.9453 - 314ms/step          \r\n",
      "Epoch 85/256\r\n",
      "step 59/59 [==============================] - loss: 0.1619 - acc: 0.9502 - 311ms/step          \r\n",
      "Epoch 86/256\r\n",
      "step 59/59 [==============================] - loss: 0.2283 - acc: 0.9486 - 306ms/step          \r\n",
      "Epoch 87/256\r\n",
      "step 59/59 [==============================] - loss: 0.4468 - acc: 0.9453 - 315ms/step          \r\n",
      "Epoch 88/256\r\n",
      "step 59/59 [==============================] - loss: 0.1211 - acc: 0.9373 - 316ms/step          \r\n",
      "Epoch 89/256\r\n",
      "step 59/59 [==============================] - loss: 0.4103 - acc: 0.9518 - 327ms/step          \r\n",
      "Epoch 90/256\r\n",
      "step 59/59 [==============================] - loss: 0.0689 - acc: 0.9502 - 311ms/step          \r\n",
      "Epoch 91/256\r\n",
      "step 59/59 [==============================] - loss: 0.1685 - acc: 0.9518 - 310ms/step          \r\n",
      "Epoch 92/256\r\n",
      "step 59/59 [==============================] - loss: 0.3555 - acc: 0.9443 - 306ms/step          \r\n",
      "Epoch 93/256\r\n",
      "step 59/59 [==============================] - loss: 0.0434 - acc: 0.9223 - 307ms/step          \r\n",
      "Epoch 94/256\r\n",
      "step 59/59 [==============================] - loss: 0.2546 - acc: 0.9652 - 316ms/step          \r\n",
      "Epoch 95/256\r\n",
      "step 59/59 [==============================] - loss: 0.5010 - acc: 0.9448 - 311ms/step          \r\n",
      "Epoch 96/256\r\n",
      "step 59/59 [==============================] - loss: 0.6611 - acc: 0.9459 - 320ms/step          \r\n",
      "Epoch 97/256\r\n",
      "step 59/59 [==============================] - loss: 0.2871 - acc: 0.9250 - 311ms/step          \r\n",
      "Epoch 98/256\r\n",
      "step 59/59 [==============================] - loss: 0.0850 - acc: 0.9614 - 318ms/step          \r\n",
      "Epoch 99/256\r\n",
      "step 59/59 [==============================] - loss: 0.0726 - acc: 0.9459 - 314ms/step          \r\n",
      "Epoch 100/256\r\n",
      "step 59/59 [==============================] - loss: 0.5245 - acc: 0.9609 - 312ms/step          \r\n",
      "Epoch 101/256\r\n",
      "step 59/59 [==============================] - loss: 0.0480 - acc: 0.9587 - 314ms/step          \r\n",
      "Epoch 102/256\r\n",
      "step 59/59 [==============================] - loss: 0.0177 - acc: 0.9759 - 315ms/step          \r\n",
      "Epoch 103/256\r\n",
      "step 59/59 [==============================] - loss: 0.0235 - acc: 0.9727 - 314ms/step          \r\n",
      "Epoch 104/256\r\n",
      "step 59/59 [==============================] - loss: 0.5327 - acc: 0.9475 - 313ms/step          \r\n",
      "Epoch 105/256\r\n",
      "step 59/59 [==============================] - loss: 0.0368 - acc: 0.9250 - 313ms/step          \r\n",
      "Epoch 106/256\r\n",
      "step 59/59 [==============================] - loss: 0.0039 - acc: 0.9652 - 314ms/step          \r\n",
      "Epoch 107/256\r\n",
      "step 59/59 [==============================] - loss: 0.8434 - acc: 0.9571 - 315ms/step          \r\n",
      "Epoch 108/256\r\n",
      "step 59/59 [==============================] - loss: 0.2739 - acc: 0.9196 - 310ms/step          \r\n",
      "Epoch 109/256\r\n",
      "step 59/59 [==============================] - loss: 0.1142 - acc: 0.9373 - 314ms/step          \r\n",
      "Epoch 110/256\r\n",
      "step 59/59 [==============================] - loss: 0.9292 - acc: 0.9411 - 319ms/step          \r\n",
      "Epoch 111/256\r\n",
      "step 59/59 [==============================] - loss: 0.0106 - acc: 0.9282 - 312ms/step          \r\n",
      "Epoch 112/256\r\n",
      "step 59/59 [==============================] - loss: 0.1734 - acc: 0.9695 - 318ms/step          \r\n",
      "Epoch 113/256\r\n",
      "step 59/59 [==============================] - loss: 0.0099 - acc: 0.9620 - 306ms/step          \r\n",
      "Epoch 114/256\r\n",
      "step 59/59 [==============================] - loss: 0.0026 - acc: 0.9571 - 309ms/step          \r\n",
      "Epoch 115/256\r\n",
      "step 59/59 [==============================] - loss: 0.1064 - acc: 0.9657 - 312ms/step          \r\n",
      "Epoch 116/256\r\n",
      "step 59/59 [==============================] - loss: 0.0016 - acc: 0.9727 - 320ms/step          \r\n",
      "Epoch 117/256\r\n",
      "step 59/59 [==============================] - loss: 0.1271 - acc: 0.9689 - 311ms/step          \r\n",
      "Epoch 118/256\r\n",
      "step 59/59 [==============================] - loss: 0.0974 - acc: 0.9636 - 312ms/step          \r\n",
      "Epoch 119/256\r\n",
      "step 59/59 [==============================] - loss: 0.2120 - acc: 0.9711 - 318ms/step          \r\n",
      "Epoch 120/256\r\n",
      "step 59/59 [==============================] - loss: 0.2724 - acc: 0.9641 - 314ms/step          \r\n",
      "Epoch 121/256\r\n",
      "step 59/59 [==============================] - loss: 0.5442 - acc: 0.9352 - 307ms/step          \r\n",
      "Epoch 122/256\r\n",
      "step 59/59 [==============================] - loss: 0.0492 - acc: 0.9646 - 308ms/step          \r\n",
      "Epoch 123/256\r\n",
      "step 59/59 [==============================] - loss: 0.0396 - acc: 0.9673 - 319ms/step          \r\n",
      "Epoch 124/256\r\n",
      "step 59/59 [==============================] - loss: 0.1400 - acc: 0.9630 - 316ms/step          \r\n",
      "Epoch 125/256\r\n",
      "step 59/59 [==============================] - loss: 0.1185 - acc: 0.9566 - 312ms/step          \r\n",
      "Epoch 126/256\r\n",
      "step 59/59 [==============================] - loss: 0.0204 - acc: 0.9652 - 307ms/step          \r\n",
      "Epoch 127/256\r\n",
      "step 59/59 [==============================] - loss: 1.1811 - acc: 0.9652 - 311ms/step          \r\n",
      "Epoch 128/256\r\n",
      "step 59/59 [==============================] - loss: 0.2414 - acc: 0.9523 - 320ms/step          \r\n",
      "Epoch 129/256\r\n",
      "step 59/59 [==============================] - loss: 0.0252 - acc: 0.9507 - 318ms/step          \r\n",
      "Epoch 130/256\r\n",
      "step 59/59 [==============================] - loss: 0.0398 - acc: 0.9753 - 314ms/step          \r\n",
      "Epoch 131/256\r\n",
      "step 59/59 [==============================] - loss: 0.0047 - acc: 0.9759 - 318ms/step          \r\n",
      "Epoch 132/256\r\n",
      "step 59/59 [==============================] - loss: 0.1417 - acc: 0.9727 - 310ms/step          \r\n",
      "Epoch 133/256\r\n",
      "step 59/59 [==============================] - loss: 0.5197 - acc: 0.9673 - 315ms/step          \r\n",
      "Epoch 134/256\r\n",
      "step 59/59 [==============================] - loss: 0.0026 - acc: 0.9646 - 312ms/step          \r\n",
      "Epoch 135/256\r\n",
      "step 59/59 [==============================] - loss: 0.0213 - acc: 0.9812 - 309ms/step          \r\n",
      "Epoch 136/256\r\n",
      "step 59/59 [==============================] - loss: 0.3661 - acc: 0.9845 - 311ms/step          \r\n",
      "Epoch 137/256\r\n",
      "step 59/59 [==============================] - loss: 0.9821 - acc: 0.9673 - 320ms/step          \r\n",
      "Epoch 138/256\r\n",
      "step 59/59 [==============================] - loss: 0.0415 - acc: 0.9319 - 320ms/step          \r\n",
      "Epoch 139/256\r\n",
      "step 59/59 [==============================] - loss: 0.0170 - acc: 0.9534 - 308ms/step          \r\n",
      "Epoch 140/256\r\n",
      "step 59/59 [==============================] - loss: 0.2771 - acc: 0.9678 - 313ms/step          \r\n",
      "Epoch 141/256\r\n",
      "step 59/59 [==============================] - loss: 0.0398 - acc: 0.9732 - 315ms/step          \r\n",
      "Epoch 142/256\r\n",
      "step 59/59 [==============================] - loss: 0.4372 - acc: 0.9737 - 312ms/step          \r\n",
      "Epoch 143/256\r\n",
      "step 59/59 [==============================] - loss: 0.0560 - acc: 0.9732 - 315ms/step          \r\n",
      "Epoch 144/256\r\n",
      "step 59/59 [==============================] - loss: 0.1539 - acc: 0.9534 - 319ms/step          \r\n",
      "Epoch 145/256\r\n",
      "step 59/59 [==============================] - loss: 0.7294 - acc: 0.9523 - 308ms/step          \r\n",
      "Epoch 146/256\r\n",
      "step 59/59 [==============================] - loss: 0.2285 - acc: 0.9539 - 311ms/step          \r\n",
      "Epoch 147/256\r\n",
      "step 59/59 [==============================] - loss: 0.0400 - acc: 0.9421 - 309ms/step          \r\n",
      "Epoch 148/256\r\n",
      "step 59/59 [==============================] - loss: 0.1904 - acc: 0.9571 - 309ms/step          \r\n",
      "Epoch 149/256\r\n",
      "step 59/59 [==============================] - loss: 0.0202 - acc: 0.9721 - 314ms/step          \r\n",
      "Epoch 150/256\r\n",
      "step 59/59 [==============================] - loss: 0.1835 - acc: 0.9786 - 316ms/step          \r\n",
      "Epoch 151/256\r\n",
      "step 59/59 [==============================] - loss: 0.1990 - acc: 0.9818 - 309ms/step          \r\n",
      "Epoch 152/256\r\n",
      "step 59/59 [==============================] - loss: 0.1435 - acc: 0.9737 - 318ms/step          \r\n",
      "Epoch 153/256\r\n",
      "step 59/59 [==============================] - loss: 0.0228 - acc: 0.9603 - 312ms/step          \r\n",
      "Epoch 154/256\r\n",
      "step 59/59 [==============================] - loss: 0.0765 - acc: 0.9855 - 307ms/step          \r\n",
      "Epoch 155/256\r\n",
      "step 59/59 [==============================] - loss: 0.0142 - acc: 0.9807 - 312ms/step          \r\n",
      "Epoch 156/256\r\n",
      "step 59/59 [==============================] - loss: 0.0762 - acc: 0.9737 - 311ms/step          \r\n",
      "Epoch 157/256\r\n",
      "step 59/59 [==============================] - loss: 0.7882 - acc: 0.9748 - 320ms/step          \r\n",
      "Epoch 158/256\r\n",
      "step 59/59 [==============================] - loss: 0.0837 - acc: 0.9652 - 305ms/step          \r\n",
      "Epoch 159/256\r\n",
      "step 59/59 [==============================] - loss: 0.0387 - acc: 0.9845 - 304ms/step          \r\n",
      "Epoch 160/256\r\n",
      "step 59/59 [==============================] - loss: 0.2095 - acc: 0.9834 - 310ms/step          \r\n",
      "Epoch 161/256\r\n",
      "step 59/59 [==============================] - loss: 0.5212 - acc: 0.9662 - 303ms/step          \r\n",
      "Epoch 162/256\r\n",
      "step 59/59 [==============================] - loss: 0.0296 - acc: 0.9528 - 304ms/step          \r\n",
      "Epoch 163/256\r\n",
      "step 59/59 [==============================] - loss: 0.8364 - acc: 0.9368 - 311ms/step          \r\n",
      "Epoch 164/256\r\n",
      "step 59/59 [==============================] - loss: 0.3608 - acc: 0.9405 - 314ms/step          \r\n",
      "Epoch 165/256\r\n",
      "step 59/59 [==============================] - loss: 0.2543 - acc: 0.9609 - 310ms/step          \r\n",
      "Epoch 166/256\r\n",
      "step 59/59 [==============================] - loss: 0.0733 - acc: 0.9732 - 308ms/step          \r\n",
      "Epoch 167/256\r\n",
      "step 59/59 [==============================] - loss: 0.0240 - acc: 0.9780 - 306ms/step          \r\n",
      "Epoch 168/256\r\n",
      "step 59/59 [==============================] - loss: 0.0934 - acc: 0.9861 - 313ms/step          \r\n",
      "Epoch 169/256\r\n",
      "step 59/59 [==============================] - loss: 0.5313 - acc: 0.9855 - 305ms/step          \r\n",
      "Epoch 170/256\r\n",
      "step 59/59 [==============================] - loss: 0.0891 - acc: 0.9877 - 312ms/step          \r\n",
      "Epoch 171/256\r\n",
      "step 59/59 [==============================] - loss: 0.1636 - acc: 0.9877 - 316ms/step          \r\n",
      "Epoch 172/256\r\n",
      "step 59/59 [==============================] - loss: 0.0035 - acc: 0.9737 - 311ms/step          \r\n",
      "Epoch 173/256\r\n",
      "step 59/59 [==============================] - loss: 0.2061 - acc: 0.9695 - 316ms/step          \r\n",
      "Epoch 174/256\r\n",
      "step 59/59 [==============================] - loss: 0.0619 - acc: 0.9582 - 314ms/step          \r\n",
      "Epoch 175/256\r\n",
      "step 59/59 [==============================] - loss: 0.0230 - acc: 0.9550 - 311ms/step          \r\n",
      "Epoch 176/256\r\n",
      "step 59/59 [==============================] - loss: 1.4767 - acc: 0.9609 - 310ms/step          \r\n",
      "Epoch 177/256\r\n",
      "step 59/59 [==============================] - loss: 0.5445 - acc: 0.9464 - 310ms/step          \r\n",
      "Epoch 178/256\r\n",
      "step 59/59 [==============================] - loss: 0.1405 - acc: 0.9620 - 316ms/step          \r\n",
      "Epoch 179/256\r\n",
      "step 59/59 [==============================] - loss: 0.0838 - acc: 0.9716 - 312ms/step          \r\n",
      "Epoch 180/256\r\n",
      "step 59/59 [==============================] - loss: 0.0436 - acc: 0.9850 - 314ms/step          \r\n",
      "Epoch 181/256\r\n",
      "step 59/59 [==============================] - loss: 0.0114 - acc: 0.9887 - 310ms/step          \r\n",
      "Epoch 182/256\r\n",
      "step 59/59 [==============================] - loss: 0.2628 - acc: 0.9839 - 309ms/step          \r\n",
      "Epoch 183/256\r\n",
      "step 59/59 [==============================] - loss: 0.0202 - acc: 0.9770 - 313ms/step          \r\n",
      "Epoch 184/256\r\n",
      "step 59/59 [==============================] - loss: 0.4958 - acc: 0.9845 - 317ms/step          \r\n",
      "Epoch 185/256\r\n",
      "step 59/59 [==============================] - loss: 0.5187 - acc: 0.9641 - 311ms/step          \r\n",
      "Epoch 186/256\r\n",
      "step 59/59 [==============================] - loss: 0.2159 - acc: 0.9255 - 315ms/step          \r\n",
      "Epoch 187/256\r\n",
      "step 59/59 [==============================] - loss: 0.2466 - acc: 0.9678 - 308ms/step          \r\n",
      "Epoch 188/256\r\n",
      "step 59/59 [==============================] - loss: 0.0941 - acc: 0.9812 - 311ms/step          \r\n",
      "Epoch 189/256\r\n",
      "step 59/59 [==============================] - loss: 0.0520 - acc: 0.9861 - 306ms/step          \r\n",
      "Epoch 190/256\r\n",
      "step 59/59 [==============================] - loss: 0.2128 - acc: 0.9866 - 315ms/step          \r\n",
      "Epoch 191/256\r\n",
      "step 59/59 [==============================] - loss: 0.0047 - acc: 0.9727 - 316ms/step          \r\n",
      "Epoch 192/256\r\n",
      "step 59/59 [==============================] - loss: 0.3369 - acc: 0.9855 - 315ms/step          \r\n",
      "Epoch 193/256\r\n",
      "step 59/59 [==============================] - loss: 0.0099 - acc: 0.9780 - 307ms/step          \r\n",
      "Epoch 194/256\r\n",
      "step 59/59 [==============================] - loss: 0.0743 - acc: 0.9737 - 317ms/step          \r\n",
      "Epoch 195/256\r\n",
      "step 59/59 [==============================] - loss: 0.1285 - acc: 0.9689 - 313ms/step          \r\n",
      "Epoch 196/256\r\n",
      "step 59/59 [==============================] - loss: 0.0093 - acc: 0.9544 - 312ms/step          \r\n",
      "Epoch 197/256\r\n",
      "step 59/59 [==============================] - loss: 0.0626 - acc: 0.9753 - 306ms/step          \r\n",
      "Epoch 198/256\r\n",
      "step 59/59 [==============================] - loss: 0.0202 - acc: 0.9770 - 317ms/step          \r\n",
      "Epoch 199/256\r\n",
      "step 59/59 [==============================] - loss: 0.0368 - acc: 0.9759 - 304ms/step          \r\n",
      "Epoch 200/256\r\n",
      "step 59/59 [==============================] - loss: 0.1916 - acc: 0.9812 - 311ms/step          \r\n",
      "Epoch 201/256\r\n",
      "step 59/59 [==============================] - loss: 0.0023 - acc: 0.9861 - 310ms/step          \r\n",
      "Epoch 202/256\r\n",
      "step 59/59 [==============================] - loss: 0.0904 - acc: 0.9898 - 301ms/step          \r\n",
      "Epoch 203/256\r\n",
      "step 59/59 [==============================] - loss: 2.8562e-04 - acc: 0.9893 - 315ms/step      \r\n",
      "Epoch 204/256\r\n",
      "step 59/59 [==============================] - loss: 0.0134 - acc: 0.9850 - 310ms/step          \r\n",
      "Epoch 205/256\r\n",
      "step 59/59 [==============================] - loss: 0.1536 - acc: 0.9850 - 324ms/step          \r\n",
      "Epoch 206/256\r\n",
      "step 59/59 [==============================] - loss: 0.0046 - acc: 0.9780 - 313ms/step          \r\n",
      "Epoch 207/256\r\n",
      "step 59/59 [==============================] - loss: 0.0094 - acc: 0.9866 - 310ms/step          \r\n",
      "Epoch 208/256\r\n",
      "step 59/59 [==============================] - loss: 0.3355 - acc: 0.9845 - 312ms/step          \r\n",
      "Epoch 209/256\r\n",
      "step 59/59 [==============================] - loss: 0.0040 - acc: 0.9812 - 316ms/step          \r\n",
      "Epoch 210/256\r\n",
      "step 59/59 [==============================] - loss: 0.3239 - acc: 0.9775 - 317ms/step          \r\n",
      "Epoch 211/256\r\n",
      "step 59/59 [==============================] - loss: 0.0136 - acc: 0.9443 - 313ms/step          \r\n",
      "Epoch 212/256\r\n",
      "step 59/59 [==============================] - loss: 0.0269 - acc: 0.9743 - 312ms/step          \r\n",
      "Epoch 213/256\r\n",
      "step 59/59 [==============================] - loss: 0.8685 - acc: 0.9855 - 310ms/step          \r\n",
      "Epoch 214/256\r\n",
      "step 59/59 [==============================] - loss: 0.2801 - acc: 0.9641 - 309ms/step          \r\n",
      "Epoch 215/256\r\n",
      "step 59/59 [==============================] - loss: 0.1370 - acc: 0.9780 - 314ms/step          \r\n",
      "Epoch 216/256\r\n",
      "step 59/59 [==============================] - loss: 0.0393 - acc: 0.9796 - 310ms/step          \r\n",
      "Epoch 217/256\r\n",
      "step 59/59 [==============================] - loss: 0.0018 - acc: 0.9871 - 315ms/step          \r\n",
      "Epoch 218/256\r\n",
      "step 59/59 [==============================] - loss: 0.0147 - acc: 0.9829 - 310ms/step          \r\n",
      "Epoch 219/256\r\n",
      "step 59/59 [==============================] - loss: 0.2078 - acc: 0.9904 - 311ms/step          \r\n",
      "Epoch 220/256\r\n",
      "step 59/59 [==============================] - loss: 0.1588 - acc: 0.9812 - 313ms/step          \r\n",
      "Epoch 221/256\r\n",
      "step 59/59 [==============================] - loss: 0.0592 - acc: 0.9689 - 313ms/step          \r\n",
      "Epoch 222/256\r\n",
      "step 59/59 [==============================] - loss: 0.0022 - acc: 0.9721 - 306ms/step          \r\n",
      "Epoch 223/256\r\n",
      "step 59/59 [==============================] - loss: 0.0024 - acc: 0.9802 - 317ms/step          \r\n",
      "Epoch 224/256\r\n",
      "step 59/59 [==============================] - loss: 0.3447 - acc: 0.9775 - 308ms/step          \r\n",
      "Epoch 225/256\r\n",
      "step 59/59 [==============================] - loss: 0.2024 - acc: 0.9507 - 318ms/step          \r\n",
      "Epoch 226/256\r\n",
      "step 59/59 [==============================] - loss: 0.1614 - acc: 0.9652 - 314ms/step          \r\n",
      "Epoch 227/256\r\n",
      "step 59/59 [==============================] - loss: 0.0943 - acc: 0.9711 - 312ms/step          \r\n",
      "Epoch 228/256\r\n",
      "step 59/59 [==============================] - loss: 0.4129 - acc: 0.9523 - 314ms/step          \r\n",
      "Epoch 229/256\r\n",
      "step 59/59 [==============================] - loss: 0.0764 - acc: 0.9491 - 310ms/step          \r\n",
      "Epoch 230/256\r\n",
      "step 59/59 [==============================] - loss: 0.0437 - acc: 0.9748 - 312ms/step          \r\n",
      "Epoch 231/256\r\n",
      "step 59/59 [==============================] - loss: 0.0351 - acc: 0.9855 - 300ms/step          \r\n",
      "Epoch 232/256\r\n",
      "step 59/59 [==============================] - loss: 0.0220 - acc: 0.9823 - 317ms/step          \r\n",
      "Epoch 233/256\r\n",
      "step 59/59 [==============================] - loss: 0.0255 - acc: 0.9882 - 315ms/step          \r\n",
      "Epoch 234/256\r\n",
      "step 59/59 [==============================] - loss: 0.5423 - acc: 0.9920 - 311ms/step          \r\n",
      "Epoch 235/256\r\n",
      "step 59/59 [==============================] - loss: 0.1024 - acc: 0.9791 - 306ms/step          \r\n",
      "Epoch 236/256\r\n",
      "step 59/59 [==============================] - loss: 0.0494 - acc: 0.9753 - 306ms/step          \r\n",
      "Epoch 237/256\r\n",
      "step 59/59 [==============================] - loss: 0.0606 - acc: 0.9818 - 314ms/step          \r\n",
      "Epoch 238/256\r\n",
      "step 59/59 [==============================] - loss: 0.0031 - acc: 0.9845 - 313ms/step          \r\n",
      "Epoch 239/256\r\n",
      "step 59/59 [==============================] - loss: 0.0156 - acc: 0.9898 - 325ms/step          \r\n",
      "Epoch 240/256\r\n",
      "step 59/59 [==============================] - loss: 0.0417 - acc: 0.9887 - 313ms/step          \r\n",
      "Epoch 241/256\r\n",
      "step 59/59 [==============================] - loss: 0.0538 - acc: 0.9882 - 318ms/step          \r\n",
      "Epoch 242/256\r\n",
      "step 59/59 [==============================] - loss: 0.0043 - acc: 0.9877 - 308ms/step          \r\n",
      "Epoch 243/256\r\n",
      "step 59/59 [==============================] - loss: 0.0267 - acc: 0.9914 - 310ms/step          \r\n",
      "Epoch 244/256\r\n",
      "step 59/59 [==============================] - loss: 0.0864 - acc: 0.9904 - 313ms/step          \r\n",
      "Epoch 245/256\r\n",
      "step 59/59 [==============================] - loss: 0.0792 - acc: 0.9845 - 308ms/step          \r\n",
      "Epoch 246/256\r\n",
      "step 59/59 [==============================] - loss: 0.0033 - acc: 0.9887 - 316ms/step          \r\n",
      "Epoch 247/256\r\n",
      "step 59/59 [==============================] - loss: 0.0695 - acc: 0.9914 - 314ms/step          \r\n",
      "Epoch 248/256\r\n",
      "step 59/59 [==============================] - loss: 0.6476 - acc: 0.9823 - 311ms/step          \r\n",
      "Epoch 249/256\r\n",
      "step 59/59 [==============================] - loss: 0.0015 - acc: 0.9684 - 315ms/step          \r\n",
      "Epoch 250/256\r\n",
      "step 59/59 [==============================] - loss: 0.0171 - acc: 0.9662 - 315ms/step          \r\n",
      "Epoch 251/256\r\n",
      "step 59/59 [==============================] - loss: 0.1714 - acc: 0.9609 - 315ms/step          \r\n",
      "Epoch 252/256\r\n",
      "step 59/59 [==============================] - loss: 0.4891 - acc: 0.9678 - 321ms/step          \r\n",
      "Epoch 253/256\r\n",
      "step 59/59 [==============================] - loss: 0.0118 - acc: 0.9721 - 317ms/step          \r\n",
      "Epoch 254/256\r\n",
      "step 59/59 [==============================] - loss: 0.2580 - acc: 0.9898 - 314ms/step          \r\n",
      "Epoch 255/256\r\n",
      "step 59/59 [==============================] - loss: 0.0190 - acc: 0.9877 - 307ms/step          \r\n",
      "Epoch 256/256\r\n",
      "step 59/59 [==============================] - loss: 4.4563e-04 - acc: 0.9920 - 307ms/step      \r\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "model.fit(train_dataset, \n",
    "          epochs=256,\n",
    "          batch_size=32,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4065570-dba4-4386-a28e-c2e24cd5e91c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T09:39:09.698652Z",
     "iopub.status.busy": "2023-06-18T09:39:09.698053Z",
     "iopub.status.idle": "2023-06-18T09:39:40.390078Z",
     "shell.execute_reply": "2023-06-18T09:39:40.389030Z",
     "shell.execute_reply.started": "2023-06-18T09:39:09.698622Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval begin...\r\n",
      "step 1866/1866 [==============================] - loss: 6.3244e-04 - acc: 0.9957 - 16ms/step          \r\n",
      "Eval samples: 1866\r\n",
      "{'loss': [0.00063244364], 'acc': 0.9957127545551983}\r\n"
     ]
    }
   ],
   "source": [
    "# 用 evaluate 在训练集上对模型进行验证\n",
    "eval_result = model.evaluate(train_dataset, verbose=1)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "691bae81-40f1-4987-83d8-747c3bff9471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T09:39:40.391946Z",
     "iopub.status.busy": "2023-06-18T09:39:40.391494Z",
     "iopub.status.idle": "2023-06-18T09:39:40.409348Z",
     "shell.execute_reply": "2023-06-18T09:39:40.408430Z",
     "shell.execute_reply.started": "2023-06-18T09:39:40.391916Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 224, 224)\r\n",
      "200\r\n"
     ]
    }
   ],
   "source": [
    "class InferDataset(Dataset):\n",
    "    def __init__(self, data_dir, image_paths, transform=None):\n",
    "        \"\"\"\n",
    "        步骤二：实现 __init__ 函数，初始化数据集，将样本映射到列表中\n",
    "        \"\"\"\n",
    "        super(InferDataset, self).__init__()\n",
    "        self.data_list = []\n",
    "        with open(image_paths,encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                image_path = test_data_dir+'/'+line.strip('\\n')\n",
    "                self.data_list.append(image_path)\n",
    "        # 2. 传入定义好的数据处理方法，作为自定义数据集类的一个属性\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        步骤三：实现 __getitem__ 函数，定义指定 index 时如何获取数据，并返回单条数据（样本数据、对应的标签）\n",
    "        \"\"\"\n",
    "        image_path = self.data_list[index]\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        image = image.astype('float32')\n",
    "        # 3. 应用数据处理方法到图像上\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        步骤四：实现 __len__ 函数，返回数据集的样本总数\n",
    "        \"\"\"\n",
    "        return len(self.data_list)\n",
    "\n",
    "transform_test = Compose([\n",
    "    Resize(size=(224,224)),\n",
    "    Normalize(mean=[127.5, 127.5, 127.5], std=[127.5, 127.5, 127.5],data_format='HWC'),\n",
    "    Transpose()])\n",
    "#加载测试集\n",
    "test_dataset= InferDataset(test_data_dir,test_path,transform_test)\n",
    "print(test_dataset.__getitem__(0).shape)\n",
    "print(test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e827b41-e1cc-4d78-8421-cd6e515725f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T09:39:40.410801Z",
     "iopub.status.busy": "2023-06-18T09:39:40.410548Z",
     "iopub.status.idle": "2023-06-18T09:39:43.510103Z",
     "shell.execute_reply": "2023-06-18T09:39:43.509289Z",
     "shell.execute_reply.started": "2023-06-18T09:39:40.410778Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict begin...\r\n",
      "step 200/200 [==============================] - 15ms/step          \r\n",
      "Predict samples: 200\r\n"
     ]
    }
   ],
   "source": [
    "test_result = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01707698-b7e3-4620-a7c3-184ff7ff88dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T09:39:43.511688Z",
     "iopub.status.busy": "2023-06-18T09:39:43.511305Z",
     "iopub.status.idle": "2023-06-18T09:39:43.517030Z",
     "shell.execute_reply": "2023-06-18T09:39:43.516286Z",
     "shell.execute_reply.started": "2023-06-18T09:39:43.511659Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '001.Atrophaneura_horishanus', 1: '002.Atrophaneura_varuna', 2: '003.Byasa_alcinous', 3: '004.Byasa_dasarada', 4: '005.Byasa_polyeuctes', 5: '006.Graphium_agamemnon', 6: '007.Graphium_cloanthus', 7: '008.Graphium_sarpedon', 8: '009.Iphiclides_podalirius', 9: '010.Lamproptera_curius', 10: '011.Lamproptera_meges', 11: '012.Losaria_coon', 12: '013.Meandrusa_payeni', 13: '014.Meandrusa_sciron', 14: '015.Pachliopta_aristolochiae', 15: '016.Papilio_alcmenor', 16: '017.Papilio_arcturus', 17: '018.Papilio_bianor', 18: '019.Papilio_dialis', 19: '020.Papilio_hermosanus'}\r\n"
     ]
    }
   ],
   "source": [
    "species_dict={}\n",
    "with open(spicies_path) as f:\n",
    "    for line in f:\n",
    "        a,b = line.strip(\"\\n\").split(\" \")\n",
    "        species_dict[int(a)-1]=b\n",
    "\n",
    "print(species_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1597c0bf-4811-4749-a208-04563d68f749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T09:39:43.518492Z",
     "iopub.status.busy": "2023-06-18T09:39:43.518000Z",
     "iopub.status.idle": "2023-06-18T09:39:43.523110Z",
     "shell.execute_reply": "2023-06-18T09:39:43.522462Z",
     "shell.execute_reply.started": "2023-06-18T09:39:43.518466Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('model_result.txt','w')as f:\n",
    "    for i in range(0,200):\n",
    "        f.write(species_dict[test_result[0][i].argmax()]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36908fd-94d7-4e6f-a4ed-04481bc793b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
